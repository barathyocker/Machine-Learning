{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5775f8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split \n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier   \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69c2ec76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rfeFeature(indep_X,dep_Y, n):\n",
    "        rfelist=[]\n",
    "        \n",
    "        log_model = LogisticRegression(solver='lbfgs')\n",
    "        RF = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
    "       # NB = GaussianNB()\n",
    "        DT= DecisionTreeClassifier(criterion = 'gini', max_features='sqrt',splitter='best',random_state = 0)\n",
    "        svc_model = SVC(kernel = 'linear', random_state = 0)\n",
    "        #knn = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n",
    "        rfemodellist=[log_model,svc_model,RF,DT] \n",
    "        for i in   rfemodellist:\n",
    "            print(i)\n",
    "            log_rfe = RFE(i, n)\n",
    "            log_fit = log_rfe.fit(indep_X, dep_Y)\n",
    "            log_rfe_feature=log_fit.transform(indep_X)\n",
    "            rfelist.append(log_rfe_feature)\n",
    "        return rfelist\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3ab35d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_scalar(indep_X,dep_Y):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(indep_X, dep_Y, test_size = 0.25, random_state = 0)\n",
    "        #X_train, X_test, y_train, y_test = train_test_split(indep_X,dep_Y, test_size = 0.25, random_state = 0)\n",
    "        \n",
    "        #Feature Scaling\n",
    "        #from sklearn.preprocessing import StandardScaler\n",
    "        sc = StandardScaler()\n",
    "        X_train = sc.fit_transform(X_train)\n",
    "        X_test = sc.transform(X_test)\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "def cm_prediction(classifier,X_test):\n",
    "     y_pred = classifier.predict(X_test)\n",
    "        \n",
    "        # Making the Confusion Matrix\n",
    "     from sklearn.metrics import confusion_matrix\n",
    "     cm = confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "     from sklearn.metrics import accuracy_score \n",
    "     from sklearn.metrics import classification_report \n",
    "        #from sklearn.metrics import confusion_matrix\n",
    "        #cm = confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "     Accuracy=accuracy_score(y_test, y_pred )\n",
    "        \n",
    "     report=classification_report(y_test, y_pred)\n",
    "     return  classifier,Accuracy,report,X_test,y_test,cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be83a611",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(X_train,y_train,X_test):       \n",
    "        # Fitting K-NN to the Training set\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        classifier = LogisticRegression(random_state = 0)\n",
    "        classifier.fit(X_train, y_train)\n",
    "        classifier,Accuracy,report,X_test,y_test,cm=cm_prediction(classifier,X_test)\n",
    "        return  classifier,Accuracy,report,X_test,y_test,cm      \n",
    "    \n",
    "def svm_linear(X_train,y_train,X_test):\n",
    "                \n",
    "        from sklearn.svm import SVC\n",
    "        classifier = SVC(kernel = 'linear', random_state = 0)\n",
    "        classifier.fit(X_train, y_train)\n",
    "        classifier,Accuracy,report,X_test,y_test,cm=cm_prediction(classifier,X_test)\n",
    "        return  classifier,Accuracy,report,X_test,y_test,cm\n",
    "    \n",
    "def svm_NL(X_train,y_train,X_test):\n",
    "                \n",
    "        from sklearn.svm import SVC\n",
    "        classifier = SVC(kernel = 'rbf', random_state = 0)\n",
    "        classifier.fit(X_train, y_train)\n",
    "        classifier,Accuracy,report,X_test,y_test,cm=cm_prediction(classifier,X_test)\n",
    "        return  classifier,Accuracy,report,X_test,y_test,cm\n",
    "   \n",
    "def Navie(X_train,y_train,X_test):       \n",
    "        # Fitting K-NN to the Training set\n",
    "        from sklearn.naive_bayes import GaussianNB\n",
    "        classifier = GaussianNB()\n",
    "        classifier.fit(X_train, y_train)\n",
    "        classifier,Accuracy,report,X_test,y_test,cm=cm_prediction(classifier,X_test)\n",
    "        return  classifier,Accuracy,report,X_test,y_test,cm         \n",
    "    \n",
    "    \n",
    "def knn(X_train,y_train,X_test):\n",
    "           \n",
    "        # Fitting K-NN to the Training set\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n",
    "        classifier.fit(X_train, y_train)\n",
    "        classifier,Accuracy,report,X_test,y_test,cm=cm_prediction(classifier,X_test)\n",
    "        return  classifier,Accuracy,report,X_test,y_test,cm\n",
    "def Decision(X_train,y_train,X_test):\n",
    "        \n",
    "        # Fitting K-NN to the Training set\n",
    "        from sklearn.tree import DecisionTreeClassifier\n",
    "        classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "        classifier.fit(X_train, y_train)\n",
    "        classifier,Accuracy,report,X_test,y_test,cm=cm_prediction(classifier,X_test)\n",
    "        return  classifier,Accuracy,report,X_test,y_test,cm      \n",
    "\n",
    "\n",
    "def random(X_train,y_train,X_test):\n",
    "        \n",
    "        # Fitting K-NN to the Training set\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
    "        classifier.fit(X_train, y_train)\n",
    "        classifier,Accuracy,report,X_test,y_test,cm=cm_prediction(classifier,X_test)\n",
    "        return  classifier,Accuracy,report,X_test,y_test,cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed06faf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rfe_classification(acclog,accsvml,accsvmnl,accknn,accnav,accdes,accrf): \n",
    "    \n",
    "    rfedataframe=pd.DataFrame(index=['Logistic','SVC','Random','DecisionTree'],columns=['Logistic','SVMl','SVMnl',\n",
    "                                                                                        'KNN','Navie','Decision','Random'])\n",
    "\n",
    "    for number,idex in enumerate(rfedataframe.index):\n",
    "        \n",
    "        rfedataframe['Logistic'][idex]=acclog[number]       \n",
    "        rfedataframe['SVMl'][idex]=accsvml[number]\n",
    "        rfedataframe['SVMnl'][idex]=accsvmnl[number]\n",
    "        rfedataframe['KNN'][idex]=accknn[number]\n",
    "        rfedataframe['Navie'][idex]=accnav[number]\n",
    "        rfedataframe['Decision'][idex]=accdes[number]\n",
    "        rfedataframe['Random'][idex]=accrf[number]\n",
    "    return rfedataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45a8bb15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\2155586\\AppData\\Local\\Temp\\ipykernel_18584\\1014956693.py:5: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  indep_X=df2.drop('classification_yes', 1)\n"
     ]
    }
   ],
   "source": [
    "dataset1=pd.read_csv(\"CKD.csv\",index_col=None)\n",
    "df2=dataset1\n",
    "df2 = pd.get_dummies(df2, drop_first=True)\n",
    "\n",
    "indep_X=df2.drop('classification_yes', 1)\n",
    "dep_Y=df2['classification_yes']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5b04280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression()\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "RFE.__init__() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m rfelist\u001b[38;5;241m=\u001b[39mrfeFeature(indep_X,dep_Y,\u001b[38;5;241m3\u001b[39m)       \n\u001b[0;32m      3\u001b[0m acclog\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m      4\u001b[0m accsvml\u001b[38;5;241m=\u001b[39m[]\n",
      "Cell \u001b[1;32mIn[9], line 13\u001b[0m, in \u001b[0;36mrfeFeature\u001b[1;34m(indep_X, dep_Y, n)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m   rfemodellist:\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[1;32m---> 13\u001b[0m     log_rfe \u001b[38;5;241m=\u001b[39m RFE(i, n)\n\u001b[0;32m     14\u001b[0m     log_fit \u001b[38;5;241m=\u001b[39m log_rfe\u001b[38;5;241m.\u001b[39mfit(indep_X, dep_Y)\n\u001b[0;32m     15\u001b[0m     log_rfe_feature\u001b[38;5;241m=\u001b[39mlog_fit\u001b[38;5;241m.\u001b[39mtransform(indep_X)\n",
      "\u001b[1;31mTypeError\u001b[0m: RFE.__init__() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "rfelist=rfeFeature(indep_X,dep_Y,3)       \n",
    "\n",
    "acclog=[]\n",
    "accsvml=[]\n",
    "accsvmnl=[]\n",
    "accknn=[]\n",
    "accnav=[]\n",
    "accdes=[]\n",
    "accrf=[]\n",
    "\n",
    "for i in rfelist:   \n",
    "    X_train, X_test, y_train, y_test=split_scalar(i,dep_Y)   \n",
    "    \n",
    "        \n",
    "    classifier,Accuracy,report,X_test,y_test,cm=logistic(X_train,y_train,X_test)\n",
    "    acclog.append(Accuracy)\n",
    "    \n",
    "    classifier,Accuracy,report,X_test,y_test,cm=svm_linear(X_train,y_train,X_test)  \n",
    "    accsvml.append(Accuracy)\n",
    "    \n",
    "    classifier,Accuracy,report,X_test,y_test,cm=svm_NL(X_train,y_train,X_test)  \n",
    "    accsvmnl.append(Accuracy)\n",
    "    \n",
    "    classifier,Accuracy,report,X_test,y_test,cm=knn(X_train,y_train,X_test)  \n",
    "    accknn.append(Accuracy)\n",
    "    \n",
    "    classifier,Accuracy,report,X_test,y_test,cm=Navie(X_train,y_train,X_test)  \n",
    "    accnav.append(Accuracy)\n",
    "    \n",
    "    classifier,Accuracy,report,X_test,y_test,cm=Decision(X_train,y_train,X_test)  \n",
    "    accdes.append(Accuracy)\n",
    "    \n",
    "    classifier,Accuracy,report,X_test,y_test,cm=random(X_train,y_train,X_test)  \n",
    "    accrf.append(Accuracy)\n",
    "    \n",
    "result=rfe_classification(acclog,accsvml,accsvmnl,accknn,accnav,accdes,accrf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "674b77c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression()\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "RFE.__init__() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m rfeFeature(indep_X,dep_Y, \u001b[38;5;241m3\u001b[39m)\n",
      "Cell \u001b[1;32mIn[9], line 13\u001b[0m, in \u001b[0;36mrfeFeature\u001b[1;34m(indep_X, dep_Y, n)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m   rfemodellist:\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[1;32m---> 13\u001b[0m     log_rfe \u001b[38;5;241m=\u001b[39m RFE(i, n)\n\u001b[0;32m     14\u001b[0m     log_fit \u001b[38;5;241m=\u001b[39m log_rfe\u001b[38;5;241m.\u001b[39mfit(indep_X, dep_Y)\n\u001b[0;32m     15\u001b[0m     log_rfe_feature\u001b[38;5;241m=\u001b[39mlog_fit\u001b[38;5;241m.\u001b[39mtransform(indep_X)\n",
      "\u001b[1;31mTypeError\u001b[0m: RFE.__init__() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "rfeFeature(indep_X,dep_Y, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c776a896",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
